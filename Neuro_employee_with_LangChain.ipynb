{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Решение задачи:"
      ],
      "metadata": {
        "id": "pWL8w8puo9Ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Комментарий по поводу структуры Google документа\n",
        "\n",
        "В составлении текста для двух Google документов помогал **ChatGPT**. Ему была представлена подробная информация по нейро-специалистам, расписаны их задачи и услуги, а также описана их целевая группа. Также при сооставлении промпта была определена важность выполнения моего запроса и расписана основная идея, для которой и нужно было выполнить поставленную мной задачу.\n",
        "\n",
        "Далее я попросил его структурировать документ, опираясь на представленный мной шаблон:\n",
        "```\n",
        "Заголовок 1 уровня (логическое описание, тема к которой относиться фрагмент)\n",
        "Заголовок 2 уровня (отражает смысл фрагмента или группы, в которую входит фрагмент)\n",
        "Фрагмент (из первоначального текста, либо оптимизированный chatGPT)\n",
        "```\n",
        "\n",
        "Далее я скопировал полученный результат и донастроил его в Google документе, установив в необходимых местах формат *\"Заголовок 1 уровня\"* и \"Заголовок 2 уровня\".\n",
        "\n",
        "Также в инструкции для нейро-сотрудника (в Google документе) и промпте я решил описать желаемую структуру, которая должна быть на входе языковой модели, входными данными которой является Google документ."
      ],
      "metadata": {
        "id": "ECEyiZR5LDV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Основная часть кода\n",
        "\n",
        "Сначала необходимо установить библиотеки для работы с API OpenAI, векторную базу данных ChromaDB, фреймворк LangChain для работы с LLM, библиотеку tiktoken для подсчета токенов, а также Gradio для построения интерфейса."
      ],
      "metadata": {
        "id": "lwDRdvuJo_qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai gradio tiktoken langchain langchain-openai langchain-community chromadb"
      ],
      "metadata": {
        "id": "Q1fBIGKlo7bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01be3981-7e65-4708-b4d9-0443aee2503b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.71)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=23f12f366b129e6597023da404ec693507f4af957cc8fa1430c1071e254d85f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, python-dotenv, pybase64, overrides, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, bcrypt, backoff, watchfiles, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, onnxruntime, kubernetes, dataclasses-json, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, langchain-openai, chromadb, langchain-community\n",
            "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 httptools-0.6.4 httpx-sse-0.4.1 humanfriendly-10.0 kubernetes-33.1.0 langchain-community-0.3.27 langchain-openai-0.3.28 marshmallow-3.26.1 mmh3-5.1.0 mypy-extensions-1.1.0 onnxruntime-1.22.1 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.1 pydantic-settings-2.10.1 pypika-0.48.9 python-dotenv-1.1.1 typing-inspect-0.9.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем все необходимые библиотеки:"
      ],
      "metadata": {
        "id": "gRJM0SQ6qnFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document #работа с документами в langchain\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings #эмбеддинги для OpenAI\n",
        "from langchain.vectorstores import Chroma #доступ к векторной базе данных\n",
        "from langchain.text_splitter import CharacterTextSplitter #разделение текста на куски или чанки (chunk)\n",
        "import requests #отправка запросов\n",
        "from openai import OpenAI #доступ к OpenAI\n",
        "import gradio as gr #отрисовка интерфейса с помощью grad\n",
        "import tiktoken #библиотека подсчёта токенов\n",
        "                #без запроcов к OpenAI, тем самым не тратим деньги на запросы\n",
        "import re #для работы с регулярными выражениями\n",
        "import getpass #для работы с паролями\n",
        "import os #для работы с окружением и файловой системой"
      ],
      "metadata": {
        "id": "PdZOjprTqhh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем код для запроса ввода ключа от OpenAI:"
      ],
      "metadata": {
        "id": "5g9a4WXfsqNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Введите OpenAI API Key:\")"
      ],
      "metadata": {
        "id": "LeHlhrZLsqeL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bfa7772-190a-4330-c3d7-91be8e9348ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Любую разработку необходимо начинать с проектирования, с построения ее архитектуры. И делать это необходимо исходя из поставленной задачи (технического задания).\n",
        "\n",
        "Так давайте поставим перед собой следующее *техническое задание*:\n",
        "1. В качестве интерфейса используем **Gradio**;\n",
        "2. В интерфейсе должны быть заложены 2 механизма: обучение и отправка запросов нейро-сотруднику;\n",
        "3. Реализовать предзаполненный набор нейро-сотрудников, чтобы пользователь мог выбирать, с кем происходит общение. При выборе нейро-сотрудника показать его промпт и возможный вопрос, дать возможность их редактировать;\n",
        "4. Предоставить пользователю доступ к исходным данным для обучения в виде ссылок на гугл-документы;\n",
        "5. Взаимодействие между векторной базой, LLM и пользователем организовать на фреймворке **LangChain**.\n",
        "\n",
        "Учитывая техническое задание, мы должны создать список поддерживаемых нейро-сотрудников и задать им предварительные настройки в виде: промпта, примера пользовательского запроса, с ссылкой на обучающие наборы данных.\n",
        "\n",
        "Эти данные мы выносим в отдельную переменную. А ещё лучше вынести в отдельный файл, создав файл конфигурации. Такой подход широко используется в программировании для разделения данных и кода, что позволяет изменять данные (создавать новых нейро-сотрудников), без необходимости править код:"
      ],
      "metadata": {
        "id": "bsjvSWSOhtvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "              {\n",
        "                \"doc\": \"https://docs.google.com/document/d/1eUsc_fIU5BwDkNH0akiAFf3CxpWW3K3DMnYbS1HEjLw/edit\",\n",
        "                \"prompt\": '''Ты специалист технической поддержки в интернет-магазине, к тебе могут обращаться гости магазина и его сотрудники за подсказками, ответами на их вопросы или для решения их проблем.\n",
        "                        Компания является интернет-магазином.\n",
        "                        Постарайся дать развернутый ответ, твоя задача ответить так, чтобы у тех, кто тебе обратился, больше не осталось вопросов к тебе.\n",
        "                        Отвечай по существу, без лишних эмоций и слов, от тебя нужна только точная информация.\n",
        "                        Отвечай максимально точно по документу, не придумывай ничего от себя, будь вежлив и старайся войти в положении того, кто к тебе обратился.\n",
        "                        На входе языковой модели подаются фрагменты из векторной базы-данных в виде:\n",
        "                        - Заголовок 1 уровня: общая тема фрагмента.\n",
        "                        - Заголовок 2 уровня: смысл фрагмента.\n",
        "                        - Фрагмент: исходный текст.\n",
        "                        Данные для векторной базы данных бери из этого документа: ''',\n",
        "                \"name\": \"Нейро-специалист технической поддержки интернет-магазина\",\n",
        "                \"query\": \"Напиши примеры приветствий и прощаний\"\n",
        "              },\n",
        "              {\n",
        "                \"doc\": \"https://docs.google.com/document/d/1_oTGHq7DB4o9zzW16D_3XmrCgXbt_bT46Ii-nVzgFCA/edit\",\n",
        "                \"prompt\": '''Ты специалист технической части интернет-магазина, к тебе будут обращаться весь технический отдел интернет-магазина за подсказками и ответами на их вопросы.\n",
        "                        Компания является интернет-магазином.\n",
        "                        Постарайся дать развернутый ответ, твоя задача ответить так, чтобы у тех, кто тебе обратился, больше не осталось вопросов к тебе.\n",
        "                        Отвечай по существу, без лишних эмоций и слов, от тебя нужна только точная информация.\n",
        "                        Отвечай максимально точно по документу, не придумывай ничего от себя, будь вежлив и старайся войти в положении того, кто к тебе обратился.\n",
        "                        На входе языковой модели подаются фрагменты из векторной базы-данных в виде:\n",
        "                        - Заголовок 1 уровня: общая тема фрагмента.\n",
        "                        - Заголовок 2 уровня: смысл фрагмента.\n",
        "                        - Фрагмент: исходный текст.\n",
        "                        Данные для векторной базы данных бери из этого документа: ''',\n",
        "                \"name\": \"Нейро-специалист технической части интернет-магазина\",\n",
        "                \"query\": \"Напиши основные характеристики платформы\"\n",
        "              }\n",
        "            ]"
      ],
      "metadata": {
        "id": "OxqO_jkm7SRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Этот код реализует класс *GPT* для создания AI-ассистента, работающего с документами. Вот что происходит:\n",
        "\n",
        "1. **Инициализация**: При создании объекта инициализируются атрибуты для логов, модели ИИ и векторной базы знаний, а также устанавливается подключение к OpenAI.\n",
        "\n",
        "2. **Загрузка данных**: Метод `load_search_indexes` извлекает текст из Google Docs по URL, валидирует ссылку и передает данные на обработку.\n",
        "\n",
        "3. **Обработка документов**:\n",
        "\n",
        "*   Текст разбивается на фрагменты (**chunks**) размером 1024 символа;\n",
        "*   Рассчитывается количество токенов через `tiktoken`;\n",
        "*   Создаются векторные представления текста через `OpenAIEmbeddings`;\n",
        "*   Данные сохраняются в векторную БД **Chroma**.\n",
        "\n",
        "4. Работа с запросами:\n",
        "\n",
        "*   При запросе (`answer_index`) система ищет 5 наиболее релевантных фрагментов из БД;\n",
        "*   Формирует контекстное сообщение с промптом и найденными фрагментами;\n",
        "*   Отправляет запрос в **OpenAI**, контролируя расход токенов;\n",
        "*   Возвращает ответ модели с детальной статистикой использования токенов.\n",
        "\n",
        "Класс реализует **RAG-архитектуру** (**Retrieval-Augmented Generation**), сочетая поиск по базе знаний с генерацией ответов языковой моделью, что позволяет работать с большими документами."
      ],
      "metadata": {
        "id": "x6pLMxaNnZ3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#объявляем класс нейро-сотрудника\n",
        "class GPT():\n",
        "    #объявляем конструктор класса, для передачи имени модели и инициализации атрибутов класса\n",
        "    def __init__(self, model=\"gpt-3.5-turbo\"):\n",
        "        self.log = ''               #атрибут для сбора логов (сообщений)\n",
        "        self.model = model          #атрибут для хранения выбранной модели OpenAI\n",
        "        self.search_index = None    #атрибут для хранения ссылки на базу знаний (если None, то модель не обучена)\n",
        "        self.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "        self.base_url = \"https://api.vsegpt.ru/v1\"\n",
        "        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url) #при инициализации запрашиваем ключ от OpenAI\n",
        "\n",
        "    #метод загрузки текстового документа в векторную базу знаний\n",
        "    def load_search_indexes(self, url):\n",
        "        match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url) #извлекаем document ID гугл документа из URL с помощью регулярных выражений\n",
        "\n",
        "        #если ID не найден - генерируем исключение\n",
        "        if match_ is None:\n",
        "            raise ValueError('Неверный Google Docs URL')\n",
        "\n",
        "        doc_id = match_.group(1) #первый элемент в результате поиска\n",
        "        response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt') #скачиваем гугл документ по его ID в текстовом формате\n",
        "        response.raise_for_status() #при неудачных статусах запроса будет вызвано исключение\n",
        "        text = response.text #извлекаем данные как текст\n",
        "\n",
        "        return self.create_embedding(text) #вызываем метод векторизации текста и сохранения в векторную базу данных\n",
        "\n",
        "    #подсчет числа токенов в строке по имени модели\n",
        "    def num_tokens_from_string(self, string):\n",
        "            \"\"\"Возвращает число токенов в строке\"\"\"\n",
        "            encoding = tiktoken.encoding_for_model(self.model)  #получаем кодировщик по имени модели\n",
        "            num_tokens = len(encoding.encode(string))           #расчитываем строку с помощью кодировщика\n",
        "            return num_tokens                                   #возвращаем число токенов\n",
        "\n",
        "    #метод разбора текста и его сохранение в векторную базу знаний\n",
        "    def create_embedding(self, data):\n",
        "        source_chunks = [] #список документов, полученных из фрагментов текста\n",
        "        #разделяем текст на строки по \\n (перенос на новую строку) или длине фрагмента (chunk_size=1024) с помощью сплитера\n",
        "        #chunk_overlap=0 - означает, что фрагменты не перекрываются друг с другом.\n",
        "        #если больше нуля, то захватываем дополнительное число символов от соседних чанков.\n",
        "        splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=1024, chunk_overlap=0)\n",
        "\n",
        "        #применяем splitter (функцию расщепления) к данным и перебираем все получившиеся чанки (фрагменты)\n",
        "        for chunk in splitter.split_text(data):\n",
        "            source_chunks.append(Document(page_content=chunk, metadata={})) #LangChain работает с документами, поэтому из текстовых чанков мы создаем фрагменты документов\n",
        "\n",
        "        count_token = self.num_tokens_from_string(' '.join([x.page_content for x in source_chunks])) #подсчет числа токенов в документах без запроса к OpenAI (экономим денежные средства)\n",
        "        self.log += f'Количество токенов в документе : {count_token}\\n' #вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "\n",
        "        embeddings = OpenAIEmbeddings(openai_api_key=self.api_key, openai_api_base=self.base_url)\n",
        "        self.search_index = Chroma.from_documents(source_chunks, embeddings) #создание индексов документа. Применяем к нашему списку документов эмбеддингов OpenAi и в таком виде загружаем в базу ChromaDB\n",
        "        self.log += f'Данные из документа загружены в в векторную базу данных\\n' #вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "\n",
        "        return self.search_index #возвращаем ссылку на базу данных\n",
        "\n",
        "    #демонстрация более аккуратного расчета числа токенов в зависимости от модели\n",
        "    def num_tokens_from_messages(self, messages, model):\n",
        "        \"\"\"Возвращает число токенов из списка сообщений\"\"\"\n",
        "        try:\n",
        "            encoding = tiktoken.encoding_for_model(model) #получаем кодировщик по имени модели\n",
        "        except KeyError:\n",
        "            print(\"Предупреждение: модель не создана. Используйте cl100k_base кодировку.\")\n",
        "            encoding = tiktoken.get_encoding(\"cl100k_base\") # сли по имени не нашли, то используем базовый для моделей OpenAI\n",
        "        #выбор модели\n",
        "        if model in {\n",
        "            \"gpt-3.5-turbo-0613\",\n",
        "            \"gpt-3.5-turbo-16k-0613\",\n",
        "            \"gpt-4-0314\",\n",
        "            \"gpt-4-32k-0314\",\n",
        "            \"gpt-4-0613\",\n",
        "            \"gpt-4-32k-0613\",\n",
        "            \"gpt-4o\",\n",
        "            \"gpt-4o-2024-05-13\"\n",
        "            }:\n",
        "            tokens_per_message = 3 #дополнительное число токенов на сообщение\n",
        "            tokens_per_name = 1    #токенов на имя\n",
        "        elif model == \"gpt-3.5-turbo-0301\":\n",
        "            tokens_per_message = 4  #каждое сообщение содержит <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "            tokens_per_name = -1  #если есть имя, то роль не указывается\n",
        "        elif \"gpt-3.5-turbo\" in model:\n",
        "            self.log += f'Внимание! gpt-3.5-turbo может обновиться в любой момент. Используйте gpt-3.5-turbo-0613. \\n'\n",
        "            return self.num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
        "        elif \"gpt-4\" in model:\n",
        "            self.log += f'Внимание! gpt-4 может обновиться в любой момент. Используйте gpt-4-0613. \\n'\n",
        "            return self.num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "        else: #исключение, если модель не поддерживается\n",
        "            raise NotImplementedError(\n",
        "                f\"\"\"num_tokens_from_messages() не реализован для модели {model}.\"\"\"\n",
        "            )\n",
        "\n",
        "        #запускаем подсчет токенов\n",
        "        num_tokens = 0                        #счетчик токенов\n",
        "        for message in messages:              #цикл по всем сообщениям\n",
        "            num_tokens += tokens_per_message  #прибовляем число токенов на каждое сообщение\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value)) #считаем токены в сообщении с помощью кодировщика\n",
        "                if key == \"name\":                     #если встретили имя\n",
        "                    num_tokens += tokens_per_name     #то добавили число токенов на\n",
        "        num_tokens += 3                               #каждый ответ оборачивается в <|start|>assistant<|message|>\n",
        "        return num_tokens                             #возвращаем число токенов\n",
        "\n",
        "\n",
        "    #метод запроса к языковой модели\n",
        "    def answer_index(self, system, topic, temp = 1):\n",
        "        #проверяем обучена ли наша модель\n",
        "        if not self.search_index:\n",
        "            self.log += 'Модель необходимо обучить! \\n'\n",
        "            return ''\n",
        "\n",
        "        docs = self.search_index.similarity_search(topic, k=5) #выборка документов по схожести с запросом из векторной базы данных, topic- строка запроса, k - число извлекаемых фрагментов\n",
        "        self.log += 'Выбираем документы по степени схожести с вопросом из векторной базы данных: \\n ' #вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "        message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'Отрывок документа №{i+1}:\\n' + doc.page_content + '\\\\n' for i, doc in enumerate(docs)])) #очищаем запрос от двойных пустых строк. Каждый фрагмент подписываем: Отрывок документа № и дальше порядковый номер\n",
        "        self.log += f'{message_content} \\n' #вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "\n",
        "        #в системную роль помещаем найденные фрагменты и промпт, в пользовательскую - вопрос от пользователя\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system + f\"{message_content}\"},\n",
        "            {\"role\": \"user\", \"content\": topic}\n",
        "        ]\n",
        "\n",
        "        self.log += f\"\\n\\nТокенов использовано на вопрос по версии TikToken: {self.num_tokens_from_messages(messages, self.model)}\\n\" #вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "\n",
        "\n",
        "        #запрос к языковой моделе\n",
        "        completion = self.client.chat.completions.create(\n",
        "            model=self.model,   #используемая модель\n",
        "            messages=messages,  #список форматированных сообщений с ролями\n",
        "            temperature=temp    #точность ответов модели\n",
        "        )\n",
        "\n",
        "\n",
        "        #вместо вывода print, мы формируем переменную log для дальнейшего вывода в gradio информации\n",
        "        self.log += '\\nСтатистика по токенам от языковой модели:\\n'\n",
        "        self.log += f'Токенов использовано всего (вопрос): {completion.usage.prompt_tokens} \\n'       #число токенов на вопрос по расчетам LLM\n",
        "        self.log += f'Токенов использовано всего (вопрос-ответ): {completion.usage.total_tokens} \\n'  #число токенов на вопрос и ответ по расчетам LLM\n",
        "\n",
        "        return completion.choices[0].message.content #возвращаем результат предсказания"
      ],
      "metadata": {
        "id": "oGDFLP5tnaU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Этот код реализует веб-интерфейс с использованием **Gradio** для взаимодействия с нейросетевым ассистентом на базе класса *GPT*. При запуске создается экземпляр класса *GPT* с моделью \"`gpt-3.5-turbo`\". Интерфейс включает выпадающий список для выбора предопределенных конфигураций данных из переменной models, при изменении которого автоматически обновляются связанные поля: отображается название выбранной конфигурации, системный промпт (с очисткой от лишних пробелов), пример пользовательского запроса и активная HTML-ссылка на обучающий документ *Google Docs*.\n",
        "\n",
        "Основное взаимодействие происходит через две кнопки: \"*Обучить модель*\" и \"*Запрос к модели*\". При нажатии первой кнопки происходит загрузка выбранного документа в векторную базу знаний через метод `load_search_indexes`, а вторая кнопка инициирует запрос к языковой модели через метод `answer_index`, используя введённый промпт и запрос пользователя. Результаты работы системы выводятся в двух соседних текстовых полях: в первом отображается ответ нейросети, во втором - технические логи процесса, включая информацию о токенах и этапах обработки данных. Весь интерфейс запускается через вызов `demo.launch()`:"
      ],
      "metadata": {
        "id": "KSKQdvNanerF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPT(\"gpt-3.5-turbo\") #объявляем экземпляр класса GPT (созданный ранее) и передаем ему в конструктор модель LLM, с которой будем работать\n",
        "\n",
        "blocks = gr.Blocks() #Gradio позволяет объединять элементы в блоки\n",
        "\n",
        "#работаем с блоком\n",
        "with blocks as demo:\n",
        "    subject = gr.Dropdown([(elem[\"name\"], index) for index, elem in enumerate(models)], label=\"Данные\") #объявляем элемент выбор из списка (с подписью Данные), список выбирает из поля name нашей переменной models\n",
        "    name = gr.Label(show_label=False) #здесь отобразится выбранное имя name из списка\n",
        "    prompt = gr.Textbox(label=\"Промт\", interactive=True) #промпт для запроса к LLM (по умолчанию поле prompt из models)\n",
        "    link = gr.HTML() #ссылка на файл обучения (по умолчанию поле doc из models)\n",
        "    query = gr.Textbox(label=\"Запрос к LLM\", interactive=True) #поле пользовательского запроса к LLM (по умолчанию поле query из models)\n",
        "\n",
        "\n",
        "    #функция на выбор нейро-сотрудника в models\n",
        "    #ей передается параметр subject - выбранное значение в поле списка\n",
        "    #а возвращаемые значения извлекаются из models\n",
        "    def onchange(dropdown):\n",
        "      return [\n",
        "          models[dropdown]['name'],                               #имя возвращается без изменения\n",
        "          re.sub('\\t+|\\s\\s+', ' ', models[dropdown]['prompt']),   #в промте удаляются двойные пробелы \\s\\s+ и табуляция \\t+\n",
        "          models[dropdown]['query'],                              #запрос возвращается без изменения\n",
        "          f\"<a target='_blank' href = '{models[dropdown]['doc']}'>Документ для обучения</a>\" #ссылка на документ оборачивается в html тег <a> (https://htmlbook.ru/html/a)\n",
        "          ]\n",
        "\n",
        "    #при изменении значения в поле списка subject, вызывается функция onchange\n",
        "    #ей передается параметр subject - выбранное значение в поле списка\n",
        "    #а возвращаемые значения устанавливаются в элементы name, prompt, query и link\n",
        "    subject.change(onchange, inputs = [subject], outputs = [name, prompt, query, link])\n",
        "\n",
        "    #строку в gradio можно разделить на столбцы (каждая кнопка в своем столбце)\n",
        "    with gr.Row():\n",
        "        train_btn = gr.Button(\"Обучить модель\")       #кнопка запуска обучения\n",
        "        request_btn = gr.Button(\"Запрос к модели\")    #кнопка отправки запроса к LLM\n",
        "\n",
        "    def train(dropdown):\n",
        "        try:\n",
        "            gpt.load_search_indexes(models[dropdown]['doc'])\n",
        "            return gpt.log\n",
        "        except Exception as e:\n",
        "            gpt.log += f\"\\nОшибка: {str(e)}\"\n",
        "            return gpt.log\n",
        "\n",
        "    #вызываем метод запроса к языковой модели из класса GPT\n",
        "    def predict(p, q):\n",
        "        result = gpt.answer_index(\n",
        "            p,\n",
        "            q\n",
        "        )\n",
        "        #возвращает список из ответа от LLM и log от класса GPT\n",
        "        return [result, gpt.log]\n",
        "\n",
        "    #выводим поля response с ответом от LLM и log (вывод сообщений работы класса GPT) на 2 колонки\n",
        "    with gr.Row():\n",
        "        response = gr.Textbox(label=\"Ответ LLM\") #текстовое поле с ответом от LLM\n",
        "        log = gr.Textbox(label=\"Логирование\")    #текстовое поле с выводом сообщений от GPT\n",
        "\n",
        "\n",
        "    #при нажатии на кнопку train_btn запускается функция обучения train_btn с параметром subject\n",
        "    #результат выполнения функции сохраняем в текстовое поле log - лог выполнения\n",
        "    train_btn.click(train, [subject], log)\n",
        "\n",
        "    #при нажатии на кнопку request_btn запускается функция отправки запроса к LLM request_btn с параметром prompt, query\n",
        "    #результат выполнения функции сохраняем в текстовые поля  response - ответ модели, log - лог выполнения\n",
        "    request_btn.click(predict, [prompt, query], [response, log])\n",
        "\n",
        "demo.launch() #запуск приложения"
      ],
      "metadata": {
        "id": "qcwRA1bznfPK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "5d6b2d90-1a04-4499-dedd-9d60f022e049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8e8ee5117bf00a2f3c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8e8ee5117bf00a2f3c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}